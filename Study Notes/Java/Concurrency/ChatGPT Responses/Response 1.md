## HowÂ **fairness**Â in aÂ `ReentrantLock`Â impactsÂ **throughput**

---

### 1ï¸âƒ£ Short, interview-ready answer

> **FairÂ `ReentrantLock`Â reduces throughput compared to unfair locks**Â because it enforces strict first-come-first-served ordering, which increases context switching and reduces opportunistic lock acquisition.

---

## 2ï¸âƒ£ What â€œfairnessâ€ means inÂ `ReentrantLock`

`ReentrantLock fairLock   = new ReentrantLock(true);   // fair ReentrantLock unfairLock = new ReentrantLock(false);  // default`

- **Fair lock**Â â†’ longest-waiting thread gets the lock next
    
- **Unfair lock**Â â†’ any thread can acquire the lock immediately
    

---

## 3ï¸âƒ£ Visual intuition

![https://miro.medium.com/1%2A76-3-w78IkOUK7ceb0F-Mg.png](https://miro.medium.com/1%2A76-3-w78IkOUK7ceb0F-Mg.png)

![https://flylib.com/books/2/558/1/html/2/images/13fig02.jpg](https://flylib.com/books/2/558/1/html/2/images/13fig02.jpg)

![https://ducmanhphan.github.io/img/Java/Multithreading/reentrant-lock/dead-lock-1.png](https://ducmanhphan.github.io/img/Java/Multithreading/reentrant-lock/dead-lock-1.png)

Think of:

- **Fair lock**Â â†’ orderly queue
    
- **Unfair lock**Â â†’ fast person cuts in line
    

---

## 4ï¸âƒ£ Why fair locks reduce throughput

### ğŸš« 1. Prevents lock barging

- Even if lock is free
    
- A new threadÂ **must wait**Â for queued threads
    

â¡ï¸ Missed opportunity for fast execution

---

### ğŸš« 2. More context switching

- Threads are parked & unparked strictly
    
- OS scheduling overhead increases
    

â¡ï¸ Less useful CPU work

---

### ğŸš« 3. Reduced cache locality

- Different threads acquire the lock each time
    
- CPU cache warms are lost
    

â¡ï¸ Slower execution

---

### ğŸš« 4. Strict ordering increases wait time

- Threads with short critical sections wait behind long ones
    

â¡ï¸ Lower lock utilization

---

## 5ï¸âƒ£ Why unfair locks have higher throughput

### âœ… Opportunistic acquisition

- Thread already running on CPU grabs lock immediately
    

### âœ… Fewer context switches

- No forced queueing
    

### âœ… Better cache locality

- Same thread may re-acquire lock quickly
    

â¡ï¸Â **Higher throughput**

---

## 6ï¸âƒ£ Concrete example

### Scenario

- 10 threads
    
- Critical section = 1 ms
    
- High contention
    

|Lock type|Throughput|
|---|---|
|Unfair|~900 ops/sec|
|Fair|~500 ops/sec|

Latency fairness improves, throughput drops.

---

## 7ï¸âƒ£ When fairness is useful (despite lower throughput)

UseÂ **fair locks**Â when:

- Starvation must be avoided
    
- Predictable latency is required
    
- Real-time / scheduling-sensitive systems
    

UseÂ **unfair locks**Â when:

- Maximum throughput is needed
    
- Short critical sections
    
- High contention systems
    

---

## 8ï¸âƒ£ Interview-ready comparison table

|Aspect|Fair lock|Unfair lock|
|---|---|---|
|Starvation|âŒ Prevented|âš ï¸ Possible|
|Throughput|âŒ Lower|âœ… Higher|
|Latency predictability|âœ… High|âŒ Low|
|Context switching|High|Low|

---

## 9ï¸âƒ£ One perfect interview sentence

> Fairness in aÂ `ReentrantLock`Â enforces first-come-first-served access, which improves predictability and prevents starvation but reduces throughput due to increased context switching and loss of opportunistic lock acquisition.

---

## ğŸ”Ÿ Golden rule to remember

> **Fairness improves correctness and predictability, not performance.**

---

## 1ï¸âƒ£1ï¸âƒ£ Bonus interview insight (advanced)

EvenÂ **fair locks are not perfectly fair**:

- JVM allows limited barging
    
- Fairness applies only to queued threads
    

Mentioning this earns bonus points.